{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Classical Model\n",
    "\n",
    "## The Classical Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Classical Assumptions** must be met in order for OLS estimators to be the best available. They are:\n",
    "\n",
    "1. The regression model is linear, correctly specified and has an additive error term\n",
    "2. The error term has a zero population mean\n",
    "3.  All explanatory variables are uncorrelated with each other\n",
    "4. Observations of the error term are not correlated with each other (no serial correlation)\n",
    "5. The error term has a constant variance (no heteroskedasticity)\n",
    "6. No explanatory variable is a perfect linear function of any other explanatory variable(s) (no perfect multicollinearity)\n",
    "7. The error term is normally distributed (this assumption is optional but usually invoked)\n",
    "\n",
    "An error term satisfying Assumptions 1 - 5 is called a **classical error term**, and if Assumption 7 is added, is called a **classical normal error term**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The regression model is linear, correctly specified and has an additive error term. \n",
    "\n",
    "The model is assumed to be linear:\n",
    "\n",
    "$$\n",
    "Y_i = \\beta_0 + \\beta_1X_{1i}+\\beta_2X_{2i}+ \\cdots + \\beta_KX_{Ki} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "The underlining theory does not need to be linear, though. \n",
    "\n",
    "For example, an exponential function, \n",
    "$$Y_i = e^{\\beta_0}X_i^{\\beta_1}e^{\\epsilon_i}$$\n",
    "\n",
    "can be transformed to  \n",
    "$$ln(Y_i) = \\beta_0 + \\beta_1ln(X_i) + \\epsilon_i$$\n",
    "\n",
    "and if the variables are relabelled as $Y_i^* = ln(Y_i)$ and $X_i^*= ln(X_i)$ then the form of the equation becomes linear: \n",
    "$$Y_i^* = \\beta_0 + \\beta_1 X_i^* + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The error term has a zero-population mean\n",
    "\n",
    "Econometricians add a stochastic (random) error term to regression equations to account for variation in the dependent variable that is not explained by the model. The specific value of the error term for each observation is determined purely by chance. \n",
    "\n",
    "Classical Assumption 2 says that the mean of the error term distribution, $\\epsilon$, is zero. For a small sample, the mean is not likely to be zero but as the sample size approaches $\\infty$, the mean approaches zero.\n",
    "\n",
    "For example, to compensate for the change that the mean of the population $\\epsilon$ might not equal zero, the mean of $\\epsilon$ is forced to be zero by the existence of the constant term in the equation. Given a typical regression equation:\n",
    "$$\\begin{equation}\n",
    "Y_i = \\beta_0+\\beta_1X_i + \\epsilon_i\\end{equation}\n",
    "$$\n",
    "\n",
    "If the mean of the the stochastic term, $\\epsilon_i$ is $3$. Then, consider $\\mathbb{E}(\\epsilon -3) = 0$. Add 3 to the constant term and subtract 3 from the error term to get:\n",
    "$$\\begin{equation}\n",
    "Y_i = (\\beta_0+3)+\\beta_1X_i + (\\epsilon_i -3)\\end{equation}\n",
    "$$\n",
    "\n",
    "From $(2)$, substitute $\\beta_0^*=\\beta_0+3$ and $\\epsilon _i^* = \\epsilon_i -3$ to get\n",
    "$$\\begin{align}\n",
    "Y_i = (\\beta_0^*)+\\beta_1X_i + (\\epsilon_i^*)\n",
    "\\end{align}$$\n",
    "\n",
    "and now $(3)$ conforms to Classical Assumption 2. Hence, Classical Assumption 2 can be fulfilled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. All explanatory variables are uncorrelated with the error term\n",
    "\n",
    "It is assumed that the observed values of the explanatory variables are determined independently of the values of the error term. Explanatory variables $X$s are considered to be determined outside the context of the regression equation.\n",
    "\n",
    "If an explanatory variable and the error term were correlated with each other, the OLS estimates would likely to attribute to the $X$ some of the variation in $Y$ that actually came from the error term. \n",
    "\n",
    "For example, if the error term and $X$ are positively correlated, then the estimated coefficient would probably be higher than what it would have been (upwards bias) because OLS would mistakenly attribute the variation of $Y$ caused by $\\epsilon$ to $X$ instead.\n",
    "\n",
    "Classical Assumption 3 is violated most frequently when a researcher *omits an important independent variable from an equation*. If a variable is omitted then the error term will change when this omitted variable changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Observations of the error term are uncorrelated with each other\n",
    "\n",
    "The observations of the error term are drawn independently of each other. In some cases, though this assumption is unrealistic. \n",
    "\n",
    "For example, in time series models, this classical assumption states that the increase in the error term in one period due to a shock does not show up or affect, in any way, the error term in another time period. However it is unrealistic as the shock lasts a number of time periods. If, over all the observations, $\\epsilon_{t+1}$ is correlated with $\\epsilon_t$, then the error term is said to be **serially correlated (or autocorrelated)** and assumption 4 is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. The error term has a constant variance\n",
    "\n",
    "The variance of the distribution from which the observations of the error term are drawn is constant. In other words, the observations of the error term are assumed to be drawn continually from identical distributions. The alternative would be for the variance of the distribution of the error term to change for each observation or range of observations. \n",
    "\n",
    "This Classical Assumption is usually violated when observing cross-sectional data. For example, suppose I am studying about public expenditure on education of 50 states. Because New York is larger than smaller states like Nevada, it is probable that the value of the stochastic error term is bigger than that of smaller states. The amount of unexplained variation in big states like New York is likely to be larger than that in small states like Nevada. The violation of Classical Assumption 5 is called **heteroskedasticity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. No explanatory variable is a perfect linear function of any other explanatory variable(s)\n",
    "\n",
    "Perfect **collinearity** between two independent variables imply that they are really the same variable, or one is a multiple of the other, or a constant has been added to one of the variables. Because of this, the OLS estimator cannot distinguish one variable from another.\n",
    "\n",
    "Perfect multicollinearity also can occur when two independent variables always sum to a third; or when one of the explanatory variables has a variance of zero. With perfect multicollinearity, the OLS computer program will be unable to estimate the coefficients of the collinear variables.\n",
    "\n",
    "For example, in predicting the profits of a company, you use the $\\text{annual sales of tires}$ and $\\text{annual sales tax paid}$ as variables. Since tax is a proportion of sales (usually $7\\%$) then the equation violates Classical Assumption 6 as there is a case of perfect multicollinearity. Movements in one variable directly impacts movements in another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. The error term is normally distributed\n",
    "\n",
    "Classical Assumption 7 states that the observations of the error term are drawn independently from a normal distribution. This assumption is not necessary for OLS estimation but critical for **hypothesis testing**, which uses the estimated regression coefficient to investigate hypotheses about economic behaviour. \n",
    "\n",
    "Although Assumption 7 is optional, it is usually advisible to add to the assumption for two reasons:\n",
    "- The error term $\\epsilon_i$ can be thought of as a sum of several minor errors. As the number of these minor errors get larger, the distribution of the error term ends to approach the normal distribution (by Central Limit Theorem).\n",
    "- The $t$-statistic and $F$-statistic, are not truly applicable unless the error term is normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Sampling Distribution of $\\hat \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the error term follows a probability distribution, so do the estimates of $\\beta$. The probability distribution of these $\\hat{\\beta}$ values across different samples is called the **sampling distribution of $\\hat{\\beta}$**.\n",
    "\n",
    "Recall that an *estimator* is a formula like the OLS formula but an *estimate* is the value of $\\hat{\\beta}$ computed by the formula for a given sample. Since researchers usually have one sample, beginning econometricians assume that regression analysis can produce only one estimate of $\\beta$ for a given population. In reality, however, each different sample from the same population will produce a different estimate of $\\beta$. **The collection of all the possible samples has a distribution with a *mean* and a *variance*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Mean\n",
    "A desirable property of the distribution of the estimates is that the mean is equal to the true mean of the variable being estimated. An estimator which yields such an estimate is an *unbiased estimator*. In other words, an estimator, $\\hat{\\beta}$ is **unbiased** if the mean of its sampling distribution and its true value of $\\beta$ have the same value\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}(\\hat{\\beta}) = \\beta\n",
    "\\end{align}$$\n",
    "\n",
    "Only one value of $\\hat{\\beta}$ is obtained in practice but this property is useful because a single estimate drawn from an unbiased distribution is more likely to be near the true value than one taken from a distribution not centred around the true value. If an estimator produces $\\hat{\\beta}$ which does not center around the true value of $\\beta$ then the estimator is referred to be a **biased estimator**. Usually, without any other information about the distribution of the estimates, we would always rather have an unbiased estimate than a biased one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of the Variance\n",
    "Just as we would like the distribution of the $\\hat \\beta$s to be centered around the true population $\\beta$, we would also like the distribution to be as narrow (or precise) as possible. The variance of the distribution of $\\beta$ can be decreased by increasing the size of the sample. This also increases the degrees of freedom, since degrees of freedom is number of samples minus the number of parameters to estimate.\n",
    "\n",
    "The element of chance, a random occurence, is always present in estimating regression coefficients, and sometimes estimates may be far from the true value no matter how good the estimating technique. However, if the distribution is centered around the true value and has as small a variance as possible, the element of chance is less likely to induce a poor estimate. \n",
    "\n",
    "Of course, if the sampling distribution is centered around the wrong value then a lower variance implies that most of the sampling distribution of $\\hat{\\beta}$ is concentrated on the wrong value. However, if this value is not very different from the true value then the greater precision will still be valuable.\n",
    "\n",
    "One method of deciding whether this decreased variance in the distribution of the $\\hat{\\beta}$s is valuable enough to offset the bias is to compare different estimation techniques by using a measure called the **Mean Squared Error** or **MSE**. The Mean Square Error is equal to the variance plus the square of the bias. The lower the MSE, the better.\n",
    "\n",
    "As the variance of the error term increases, so does the variance of the distribution of $\\hat{\\beta}$. The reason for the increased variance of $\\hat{\\beta}$ is that with the larger variance of  $\\epsilon_i$, the more extreme values of $\\epsilon_i$ are observed with higher frequency, and the error term becomes more important in determining the value of $Y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Standard Error of $\\hat \\beta$\n",
    "Since the standard error of the estimated coefficient, $\\text{SE}(\\hat{\\beta})$ is the square root of the estimated variance of the $\\hat{\\beta}$s, it is similarly affected by the size of the sample and the other factors we have mentioned. For example, a larger sample size will cause $\\text{SE}(\\hat{\\beta})$ to fall. The larger the sample, the more precise our coefficient estimates will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gauss-Markov Theorem & Properties of OLS Estimators\n",
    "\n",
    "The **Gauss-Markov Theorem** states that given Classical Assumptions 1 through 6, the OLS estimator of $\\beta_k$ is the minimum variance estimator from all the linear unbiased estimators of $\\beta_k , k = 0, 1, 2, \\cdots, K$.\n",
    "\n",
    "It is most easily remembered that OLS is \"BLUE\", where **BLUE** stands for Best Linear Unbiased Estimator, where 'best' here means minimum variance.\n",
    "If an equation's coefficient estimation is unbiased, then \n",
    "$$\\mathbb{E}(\\hat{\\beta_k}) = \\beta_k $$\n",
    "$k = 0, 1, 2, \\cdots, K$\n",
    "\n",
    "An unbiased estimator with the smallest variance is said to be **efficient**, and that estimator is said to have the property of efficiency.\n",
    "\n",
    "Given all seven classical assumptions, the OLS coefficient estimators can be shown to have the following properties:\n",
    "\n",
    "- *They are unbiased.* In other words, $\\mathbb{E}(\\hat{\\beta}) = \\beta$. The OLS estimates of the coefficients are centered around the true population values of the parameters being estimated.\n",
    "- *They are minimum variance.* The distribution of the coefficient estimates around the true parameter is as narrowly distributed as possible for an unbiased distribution.\n",
    "- *They are consistent.* As the sample size approaches to $\\infty$, the estimates converge to the true population parameters.\n",
    "- *They are normally distributed.* The $\\hat{\\beta}$s fulfil the property $\\hat{\\beta} \\sim \\text{N}(\\beta, VAR[\\hat{\\beta}])$. Thus, various statistical tests based on the normal distribution may be applied to these estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Econometric Notation\n",
    "\n",
    "**Population Parameter (True, unobserved values)**\n",
    "\n",
    "| Name | Symbol\n",
    "|---|---|\n",
    "|Regression Coefficient | $\\beta_k$\n",
    "|Expected value of the estimated coefficient | $\\mathbb{E}(\\hat{\\beta_k})$\n",
    "|Variance of the error term | $\\sigma^2$ or $\\text{VAR} (\\epsilon_i)$\n",
    "|Standard deviation of the error term | $\\sigma$\n",
    "|Variance of the estimated coefficient| $\\sigma^2(\\hat{\\beta_k})$ or $VAR(\\hat{\\beta_k})$\n",
    "|Standard deviation of the estimated coefficient| $\\sigma_{\\hat{\\beta_k}}$ or $\\sigma(\\hat{\\beta_k})$\n",
    "|Error or Disturbance Term | $\\epsilon_i$\n",
    "\n",
    "\n",
    "**Estimate (Observed from sample)**\n",
    "\n",
    "| Name | Symbol\n",
    "|---|---|\n",
    "|Estimated Regression Coefficient | $\\hat{\\beta_k}$\n",
    "|Estimated Variance of the error term | $s^2 \\text{ or } \\hat{\\sigma}^2$\n",
    "|Standard error | s $\\text{ or SE}$\n",
    "|Estimated Variance of the estimated coefficient | $s^2(\\hat{\\beta_k})$  or  $\\widehat{\\text{VAR}}(\\hat{\\beta_k})$\n",
    "|Standard error of the estimated coefficient | $\\sigma^2(\\hat{\\beta_k})$  or  $\\text{SE}(\\hat{\\beta_k})$\n",
    "|Residual (estimate of error in a loose sense) | $e_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
