{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Single-Independent-Variable Models with OLS\n",
    "\n",
    "The objective of regression analysis is to start from a theoretical equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\n",
    "\\end{align}$$\n",
    "\n",
    "and, through the use of data, arrive at the estimated equation:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{Y_i} = \\hat{\\beta_0}+ \\hat{\\beta_1}X_i \\end{align}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ is a sample estimate of the population value. In the case of $Y$, the \"true population value\" is $\\mathbb E[Y|X]$\n",
    "\n",
    "The most widely used method to obtain these estimates is **Ordinary Least Squares (OLS)**. OLS is a regression estimation technique that calculates  $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to minimize the sum of the squared residuals. Mathematically, given $N$ observations,\n",
    "\n",
    "$$\\begin{align}\\text{Objective of OLS is }\\min \\sum^{N}_{i=1}{e_i^2}\\end{align}$$\n",
    "\n",
    "for $i = 1, 2, \\cdots, N$\n",
    "\n",
    "The sum of squared residuals is also expressed as $\\sum (Y_i-\\hat{Y_i})^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use OLS?\n",
    "There are at least three reasons for using OLS to estimate regression models. First, it is relatively easy to use. Other models use iterative models while OLS estimates can be calculated easily by hand, using formulae. \n",
    "\n",
    "Second, the goal of minimizing $\\sum{e_i^2}$ is appropriate from a theoretical point of view as it can take into account positive or negative deviations of ($Y_i - \\hat{Y_i}$). \n",
    "\n",
    "Finally, OLS estimates have a number of useful characteristics. The sum of the residuals is *exactly* zero and OLS can be shown to be the 'best' estimator under a set of specific assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "\n",
    "An **estimator** is a mathematical technique that is applied to a sample of data to produce real-world **estimates** of the true population regression coefficients (or other parameters). Thus, OLS is an **estimator** and $\\beta$s produced by OLS are called **estimates**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does OLS work?\n",
    "\n",
    "OLS selects estimates of $\\beta_0$ and $\\beta_1$ that minimizes the squared residuals, summed over all the sample data points. \n",
    "\n",
    "For an equation with one variable, the coefficients are\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat{\\beta_1} = \n",
    "\\frac{\\sum^N_{i=1}\\begin{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "X_i-\\bar{X}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "Y_i-\\bar{Y}\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}}\n",
    "{\\sum^N_{i=1}\\begin{pmatrix}X_i - \\bar{X}\\end{pmatrix}^2}\\end{align}\n",
    "$$\n",
    "\n",
    "and, given the estimate of $\\beta_1$,\n",
    "\n",
    "$$\\begin{align}\\hat{\\beta_0}=\\bar{Y}-\\hat{\\beta_1}\\bar{X}\\end{align}$$\n",
    "\n",
    "where $\\bar{X}$ is the mean of $X$, or $\\frac {\\sum X_i}{N}$ and $\\bar{Y}$ is the mean of $Y$, or $\\frac {\\sum Y_i}{N}$. For each data set, we obtain different estimates of $\\beta_0$ and $\\beta_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of OLS Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS minimizes the sum of squared residuals or\n",
    "\n",
    "$$\\begin{align}\\min \\sum^{N}_{i=1}{e_i^2} = \\min \\sum^{N}_{i=1}(Y_i-\\hat{Y_i})^2\\end{align}$$\n",
    "\n",
    "and since\n",
    "\n",
    "$$\\begin{align}\\hat{Y_i} = \\hat{\\beta_0}+ \\hat{\\beta_1}X_i \\end{align}$$\n",
    "\n",
    "substituting $(7)$ into RHS of $(6)$ yields the objective function:\n",
    "\n",
    "$$\\begin{align}\\min \\sum_{i=1}^N e_i^2 = \\min \\sum_{i=1}^N(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2 \\end{align}$$\n",
    "\n",
    "Differentiating $(8)$ w.r.t. $\\hat{\\beta_0}$,\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial} {\\partial\\hat{\\beta_0}}\\sum_{i=1}^N(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2 = -2\\sum_{i=1}^N (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)\\end{align}$$\n",
    "\n",
    "Setting RHS of $(9)$ to $0$ and solving for $\\hat{\\beta_0}$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 &= -2\\sum_{i=1}^N (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i) \\\\\n",
    "0 &= \\sum_{i=1}^N (Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i) \\\\\n",
    "0 &= \\sum_{i=1}^N Y_i -N\\hat{\\beta_0}  - \\hat{\\beta_1}\\sum_{i=1}^N X_i \\\\\n",
    " N\\hat{\\beta_0} &= \\sum_{i=1}^N Y_i - \\hat{\\beta_1}\\sum_{i=1}^N X_i \\\\\n",
    "\\end{align*}$$\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\hat{\\beta_0} &= \\frac{\\sum_{i=1}^N Y_i - \\hat{\\beta_1}\\sum_{i=1}^N X_i}{N} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Since $N\\bar{Y} = \\sum_{i=1}^N Y_i$ and $N\\bar{X} = \\sum_{i=1}^N X_i$, substitute this into $(10)$ finally yields:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\hat{\\beta_0} &= \\frac{\\sum_{i=1}^N Y_i - \\hat{\\beta_1}\\sum_{i=1}^N X_i}{N} \\\\\n",
    "  \\hat{\\beta_0}  &= \\frac{N\\bar{Y} - \\hat{\\beta_1}N\\bar{X}}{N}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta_0}  &= \\bar{Y} - \\hat{\\beta_1}\\bar{X} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\diamond\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, differentiating $(8)$ w.r.t. $\\hat{\\beta_1}$,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial} {\\hat{\\beta_1}}\\sum_{i=1}^N(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)^2 = -2\\sum_{i=1}^N X_i(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Setting RHS of $(12)$ to $0$ and solving for $\\hat{\\beta_1}$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "0 &= -2\\sum_{i=1}^N X_i(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)\\\\\n",
    "0 &= \\sum_{i=1}^N X_i(Y_i-\\hat{\\beta_0}-\\hat{\\beta_1}X_i)\\\\\n",
    "0 &= \\sum_{i=1}^N X_iY_i- \\hat{\\beta_0}\\sum_{i=1}^NX_i- \\hat{\\beta_1}\\sum_{i=1}^NX^2_i\n",
    "\\end{align*}$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta_1}\\sum_{i=1}^NX^2_i &= \\sum_{i=1}^N X_iY_i- \\hat{\\beta_0}\\sum_{i=1}^NX_i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substitute $(11)$ or $\\hat{\\beta_0}  = \\bar{Y} - \\hat{\\beta_1}\\bar{X}$ into the $(13)$ yields:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{\\beta_1}\\sum_{i=1}^NX^2_i &= \\sum_{i=1}^N X_iY_i- (\\bar{Y} - \\hat{\\beta_1}\\bar{X})\\sum_{i=1}^NX_i \\\\\n",
    "\\hat{\\beta_1}\\sum_{i=1}^NX^2_i &= \\sum_{i=1}^N X_iY_i- \\bar{Y}\\sum_{i=1}^NX_i + \\hat{\\beta_1}\\bar{X}\\sum_{i=1}^NX_i \\\\\n",
    "\\hat{\\beta_1}\\sum_{i=1}^NX^2_i &= \\sum_{i=1}^N X_iY_i- N\\bar{Y}\\bar{X} + N\\hat{\\beta_1}\\bar{X}\\bar{X} \\\\\n",
    "\\hat{\\beta_1}\\sum_{i=1}^NX^2_i - N\\hat{\\beta_1}(\\bar{X})^2 &= \\sum_{i=1}^N X_iY_i- N\\bar{X}\\bar{Y} \\\\\n",
    "\\hat{\\beta_1}\\begin{bmatrix}\\sum_{i=1}^NX^2_i - N(\\bar{X})^2\\end{bmatrix}&= \\sum_{i=1}^N X_iY_i- N\\bar{X}\\bar{Y} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{\\beta_1} = \\frac{\\sum_{i=1}^N X_iY_i- N\\bar{X}\\bar{Y} }{\\sum_{i=1}^NX^2_i - N(\\bar{X})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For the numerator of $(14)$,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N[(X_i -\\bar{X})(Y_i -\\bar{Y})]&=\\sum_{i=1}^N[X_i Y_i-\\bar{X}Y_i-X_i\\bar{Y}+\\bar{X}\\bar{Y}]\n",
    "\\\\&=\\sum_{i=1}^N X_i Y_i- \\sum_{i=1}^N\\bar{X}Y_i- \\sum_{i=1}^NX_ i\\bar{Y}+\\sum_{i=1}^N\\bar{X}\\bar{Y}\n",
    "\\\\&=\\sum_{i=1}^N X_i Y_i- \\bar{X}\\sum_{i=1}^NY_i- \\bar{Y}\\sum_{i=1}^NX_ i+\\bar{X}\\bar{Y}\\sum_{i=1}^N 1\n",
    "\\\\&=\\sum_{i=1}^N X_i Y_i- \\bar{X} (N \\bar{Y})- \\bar{Y}(N \\bar{X})+N\\bar{X}\\bar{Y}\n",
    "\\\\&=\\sum_{i=1}^N X_iY_i- N\\bar{X}\\bar{Y}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similarly, for the denominator of $(14)$,\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^N[(X_i -\\bar{X})^2]&=\\sum_{i=1}^N[X^2_i -\\bar{X}X_i-X_i\\bar{X}+\\bar{X}\\bar{X}]\n",
    "\\\\&=\\sum_{i=1}^N X^2_i - \\sum_{i=1}^N\\bar{X}X_i- \\sum_{i=1}^NX_ i\\bar{X}+\\sum_{i=1}^N\\bar{X}\\bar{X}\n",
    "\\\\&=\\sum_{i=1}^N X^2_i - \\bar{X} (N \\bar{X})- \\bar{X}(N \\bar{X})+N\\bar{X}\\bar{X}\n",
    "\\\\&=\\sum_{i=1}^N X^2_i - N(\\bar{X})^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Substitute the LHS of the numerator and denominator into $(14)$ finally yields the OLS estimator of $\\hat{\\beta_1}$:\n",
    "$$\\begin{align}\n",
    "\\hat{\\beta_1} = \n",
    "\\frac{\\sum^N_{i=1}\\begin{bmatrix}\n",
    "\\begin{pmatrix}\n",
    "X_i-\\bar{X}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "Y_i-\\bar{Y}\n",
    "\\end{pmatrix}\n",
    "\\end{bmatrix}}\n",
    "{\\sum^N_{i=1}\\begin{pmatrix}X_i - \\bar{X}\\end{pmatrix}^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and hence these are the **normal equations** that minimise the sum of squared residuals. They are also the OLS estimators.\n",
    "\n",
    "$\\diamond$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
