{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total, Explained and Residual Sums of Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sum of squares (TSS) measures the amount of variation to be explained by the regression. It is computed by summing the squared variations of each $Y_i$ observation around the mean. Mathematically,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{TSS} = \\sum^N_{i=1}(Y_i-\\bar{Y})^2 \\label{eg2_12}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For OLS, TSS has 2 components - the variation which could be explained by the regression and the variation which cannot:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\overbrace{\\sum_i (Y_i - \\bar{Y})^2}^{\\text{Total\n",
    " Sum of Squares, TSS}} = \\overbrace{\\sum_i (\\hat{Y_i} - \\bar{Y})^2}^{\\text{Explained Sum of Squares, ESS}} +\n",
    "\\overbrace{\\sum_i e_i^2}^{\\text{Residual Sum of Squares, RSS}}\n",
    "\\end{align}\n",
    "$$\n",
    "and this is usually called the **decomposition of variance**.\n",
    "\n",
    "The first component of the RHS of $(2)$, $\\hat{Y_i} - \\bar{Y}$ is the difference between the estimated value of $Y$, $\\hat Y$ and the mean value of $Y$, $\\bar Y$. It is the amount of the squared deviation from the mean explained by the regression line, also called the **Explained Sum of Squares**. \n",
    "\n",
    "The second component is the difference between the actual value of $Y$ and the estimated value of $Y$, $\\hat Y$. This is the unexplained portion of TSS called the **Residual Sum of Squares**.\n",
    "\n",
    "The smaller the RSS relative to the ESS, the better the estimated regression line fits the data. OLS is the estimation technique which minimizes the RSS and, therefore maximizes the ESS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common measure of fit is the **coefficient of determination**, $R^2$. The coefficient of determination is the ratio of the ESS relative to the TSS\n",
    "$$\n",
    "\\begin{align*}\n",
    "R^2 &= \\frac{\\text{ESS}}{\\text{TSS}} \\\\\n",
    "&= 1- \\frac{\\text{RSS}}{\\text{TSS}} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "R^2 &= 1- \\frac{\\sum e_i^2}{\\sum (Y_i - \\bar{Y})^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The higher the $R^2$, the closer the estimated regression equation fits the sample data. Hence, $R^2$ is called a \"goodness of fit\" measure. Since TSS, ESS and RSS are all nonnegative, and $\\text{ESS}\\leq \\text{TSS}$, $R^2$ must lie in the interval $0\\leq R^2 \\leq 1$.\n",
    "\n",
    "A high value of $R^2$ demonstrates a relationship between $X$ and $Y$ that can be explained quite well by a linear regression equation. Most of the variation has been explained but there still remains a portion of the variation that is essentially random or unexplained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **simple correlation coefficient**, $r$ is a measure of the strength and direction of the linear relationship between the two variables. The interval of $r$ is $-1\\leq r \\leq 1$, and the sign of $r$ indicates the direction of the correlation between the two variables. The stronger the correlation between the two variables, the closer the *absolute value* of $r$ to 1.\\\\\n",
    "Thus, if two variables are perfectly positively correlated, $r=1$. If they are perfectly negatively correlated, $r=-1$ and if there totally no correlation between the variables, then $r=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with $R^2$ is that adding another independent variable to a particular equation *will never decrease* $R^2$. Hence, if you compare two models, say $P$ and $Q$ and model $Q$ has one more independent variable, then $Q$ will always have a better (or equal) fit as measured by $R^2$. Recall that\n",
    "$\n",
    "R^2 = \\frac{\\text{ESS}}{\\text{TSS}} \n",
    "= 1- \\frac{\\text{RSS}}{\\text{TSS}} \n",
    "= 1- \\frac{\\sum e_i^2}{\\sum (Y_i - \\bar{Y})^2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If RSS falls and TSS remains constant, then $R^2$ increases. The dependent variable has not changed so TSS stays the same. Since OLS ensures that adding a variable will not increase the summed squared residuals, RSS will only stay the same or fall.\n",
    "\n",
    "The inclusion of another variable decreases the **degrees of freedom** because it requires the estimation of another coefficient. Degrees of freedom is described as the excess number of observations ($N$) over the number of coefficients (including the intercept, commonly known as $\\beta_0$) $(K+1)$. Mathematically, $$df = N-K-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, $R^2$ is of little help if we are trying to decide whether adding a variable to an equation improves our ability to meaningfully explain the dependent variable. We use $\\bar{R^2}$ which is defined as the percentage of the variation of $Y$ around its mean, $\\bar{Y}$ that is explained by the regression equation, *adjusted for degrees of freedom*. \n",
    "$$\n",
    "\\bar{R}^2 = 1- \n",
    "\\frac{\n",
    "\t\\begin{bmatrix}\\frac{\\sum e_i^2}{N-K-1}\\end{bmatrix}\n",
    "}\n",
    "{\n",
    "\t\\begin{bmatrix}\\frac{\\sum (Y_i-\\bar{Y})^2}{N-1}\\end{bmatrix}\n",
    "}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
